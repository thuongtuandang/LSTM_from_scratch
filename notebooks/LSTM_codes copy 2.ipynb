{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is to create a dictionary of words\n",
    "word_dict = []\n",
    "for line in open('test.txt',):\n",
    "    line = line.translate(str.maketrans('','', string.punctuation))\n",
    "    tokens = line.split()\n",
    "    # Create a word dictionary\n",
    "    for word in tokens:\n",
    "        word = word.lower()\n",
    "        if word not in word_dict:\n",
    "            word_dict.append(word)\n",
    "word_dict.sort()\n",
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [153, 118, 24, 65, 81, 107, 153, 89, 52],\n",
       " 1: [0, 18, 32, 49, 153, 148, 9, 102],\n",
       " 2: [132, 115, 153, 113, 15],\n",
       " 3: [17, 135, 78, 153, 95],\n",
       " 4: [153, 149, 133, 23, 78, 153, 38, 136],\n",
       " 5: [153, 41, 80, 77, 5, 44],\n",
       " 6: [63, 19, 78, 153, 143],\n",
       " 7: [35, 86, 5, 114, 78, 153, 110],\n",
       " 8: [153, 105, 92, 151, 146, 28, 153, 60],\n",
       " 9: [122, 56, 140, 106, 153, 168, 109],\n",
       " 10: [4, 1, 12, 82, 8, 153, 45],\n",
       " 11: [153, 50, 7, 104, 66, 21, 59, 153, 84],\n",
       " 12: [165, 48, 106, 153, 129, 14],\n",
       " 13: [112, 70, 157, 164, 153, 61],\n",
       " 14: [153, 145, 160, 78, 153, 102, 136],\n",
       " 15: [153, 20, 80, 58, 169, 54, 2],\n",
       " 16: [153, 40, 155, 91, 78, 153, 119, 125],\n",
       " 17: [68, 131, 87, 5, 72, 156],\n",
       " 18: [153, 42, 63, 10, 27],\n",
       " 19: [153, 97, 30, 144, 106, 153, 76],\n",
       " 20: [153, 32, 99, 88, 78, 153, 149],\n",
       " 21: [67, 57, 138, 46, 153, 73],\n",
       " 22: [13, 62, 74, 78, 153, 136],\n",
       " 23: [153, 34, 116, 0, 50, 93],\n",
       " 24: [153, 69, 94, 31, 0, 134, 71],\n",
       " 25: [153, 96, 111, 80, 0, 33, 39],\n",
       " 26: [153, 35, 25, 0, 128, 106, 153, 14],\n",
       " 27: [153, 17, 36, 78, 153, 53, 95],\n",
       " 28: [153, 11, 90, 108, 153, 159, 169, 162, 43],\n",
       " 29: [153, 105, 103, 158, 117, 130, 78, 153, 110],\n",
       " 30: [0, 163, 22, 127, 153, 90],\n",
       " 31: [153, 47, 29, 80, 100, 78, 153, 170],\n",
       " 32: [153, 51, 141, 153, 98],\n",
       " 33: [0, 121, 6, 3, 153, 120],\n",
       " 34: [153, 142, 124, 64, 154, 153, 161],\n",
       " 35: [153, 26, 37, 101, 137],\n",
       " 36: [153, 75, 55, 153, 126, 152],\n",
       " 37: [153, 150, 83, 166, 106, 153, 147],\n",
       " 38: [153, 16, 123, 80, 79],\n",
       " 39: [153, 42, 85, 139, 78, 153, 167]}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the data into arrays of numbers based on their indices\n",
    "# Note that in order to pad sequences, we will start from index 1\n",
    "dict_arr = {}\n",
    "i = 0\n",
    "for line in open('test.txt'):\n",
    "    line = line.translate(str.maketrans('','', string.punctuation))\n",
    "    tokens = line.split()\n",
    "    line_arr = []\n",
    "    for word in tokens:\n",
    "        word = word.lower()\n",
    "        index = word_dict.index(word)\n",
    "        line_arr.append(index)\n",
    "    dict_arr[i] = line_arr\n",
    "    i += 1\n",
    "dict_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see our data array has values []\n",
    "# We will remove them all\n",
    "dict_arr = {key: value for key, value in dict_arr.items() if value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [153, 118, 24, 65, 81, 107, 153, 89, 52],\n",
       " 1: [0, 18, 32, 49, 153, 148, 9, 102],\n",
       " 2: [132, 115, 153, 113, 15],\n",
       " 3: [17, 135, 78, 153, 95],\n",
       " 4: [153, 149, 133, 23, 78, 153, 38, 136],\n",
       " 5: [153, 41, 80, 77, 5, 44],\n",
       " 6: [63, 19, 78, 153, 143],\n",
       " 7: [35, 86, 5, 114, 78, 153, 110],\n",
       " 8: [153, 105, 92, 151, 146, 28, 153, 60],\n",
       " 9: [122, 56, 140, 106, 153, 168, 109],\n",
       " 10: [4, 1, 12, 82, 8, 153, 45],\n",
       " 11: [153, 50, 7, 104, 66, 21, 59, 153, 84],\n",
       " 12: [165, 48, 106, 153, 129, 14],\n",
       " 13: [112, 70, 157, 164, 153, 61],\n",
       " 14: [153, 145, 160, 78, 153, 102, 136],\n",
       " 15: [153, 20, 80, 58, 169, 54, 2],\n",
       " 16: [153, 40, 155, 91, 78, 153, 119, 125],\n",
       " 17: [68, 131, 87, 5, 72, 156],\n",
       " 18: [153, 42, 63, 10, 27],\n",
       " 19: [153, 97, 30, 144, 106, 153, 76],\n",
       " 20: [153, 32, 99, 88, 78, 153, 149],\n",
       " 21: [67, 57, 138, 46, 153, 73],\n",
       " 22: [13, 62, 74, 78, 153, 136],\n",
       " 23: [153, 34, 116, 0, 50, 93],\n",
       " 24: [153, 69, 94, 31, 0, 134, 71],\n",
       " 25: [153, 96, 111, 80, 0, 33, 39],\n",
       " 26: [153, 35, 25, 0, 128, 106, 153, 14],\n",
       " 27: [153, 17, 36, 78, 153, 53, 95],\n",
       " 28: [153, 11, 90, 108, 153, 159, 169, 162, 43],\n",
       " 29: [153, 105, 103, 158, 117, 130, 78, 153, 110],\n",
       " 30: [0, 163, 22, 127, 153, 90],\n",
       " 31: [153, 47, 29, 80, 100, 78, 153, 170],\n",
       " 32: [153, 51, 141, 153, 98],\n",
       " 33: [0, 121, 6, 3, 153, 120],\n",
       " 34: [153, 142, 124, 64, 154, 153, 161],\n",
       " 35: [153, 26, 37, 101, 137],\n",
       " 36: [153, 75, 55, 153, 126, 152],\n",
       " 37: [153, 150, 83, 166, 106, 153, 147],\n",
       " 38: [153, 16, 123, 80, 79],\n",
       " 39: [153, 42, 85, 139, 78, 153, 167]}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now empty lines are all removed\n",
    "dict_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each word as one hot vector\n",
    "def oneHotVector(index):\n",
    "    arr = np.zeros((len(word_dict)))\n",
    "    arr[index] = 1\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 24  65  81 107 153  89  52  32  49 153 148   9 102 153 113  15  78 153\n",
      "  95 133  23  78 153  38 136  80  77   5  44  78 153 143   5 114  78 153\n",
      " 110  92 151 146  28 153  60 140 106 153 168 109  12  82   8 153  45   7\n",
      " 104  66  21  59 153  84 106 153 129  14 157 164 153  61 160  78 153 102\n",
      " 136  80  58 169  54   2 155  91  78 153 119 125  87   5  72 156  63  10\n",
      "  27  30 144 106 153  76  99  88  78 153 149 138  46 153  73  74  78 153\n",
      " 136 116   0  50  93  94  31   0 134  71 111  80   0  33  39  25   0 128\n",
      " 106 153  14  36  78 153  53  95  90 108 153 159 169 162  43 103 158 117\n",
      " 130  78 153 110  22 127 153  90  29  80 100  78 153 170 141 153  98   6\n",
      "   3 153 120 124  64 154 153 161  37 101 137  55 153 126 152  83 166 106\n",
      " 153 147 123  80  79  85 139  78 153 167]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y_true = []\n",
    "n_to_predict = 2\n",
    "for k, v in dict_arr.items():\n",
    "    l = len(v)\n",
    "    for i in range(l-n_to_predict):\n",
    "        temp = v[i:i+n_to_predict]\n",
    "        tmp = []\n",
    "        for index in temp:\n",
    "            tmp.append(oneHotVector(index))\n",
    "        X.append(tmp)\n",
    "        y_true.append(v[i+n_to_predict])\n",
    "print(np.array(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190, 2, 171)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190, 171)\n"
     ]
    }
   ],
   "source": [
    "y = to_categorical(y_true)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sigmoid and softmax\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    sigmoid_x = sigmoid(x)\n",
    "    return sigmoid_x*(1 - sigmoid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define softmax function\n",
    "def softmax(y):\n",
    "    exp_y = np.exp(y-np.max(y))\n",
    "    return exp_y/np.sum(exp_y, axis = 0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hadamard product\n",
    "def hadamard(A,B):\n",
    "    m, n = A.shape\n",
    "    c = np.zeros_like(A)\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            c[i][j] = A[i][j] * B[i][j]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, input_size, output_size, hidden_size = 64):\n",
    "        # Initialize W\n",
    "        self.Wf = np.random.rand(hidden_size, input_size)/1000\n",
    "        self.Wi = np.random.rand(hidden_size, input_size)/1000\n",
    "        self.Wc = np.random.rand(hidden_size, input_size)/1000\n",
    "        self.Wo = np.random.rand(hidden_size, input_size)/1000\n",
    "        self.Wy = np.random.rand(output_size, hidden_size)/1000\n",
    "        # Initialize U\n",
    "        self.Uf = np.random.rand(hidden_size, hidden_size)/1000\n",
    "        self.Ui = np.random.rand(hidden_size, hidden_size)/1000\n",
    "        self.Uc = np.random.rand(hidden_size, hidden_size)/1000\n",
    "        self.Uo = np.random.rand(hidden_size, hidden_size)/1000\n",
    "        # Initialize b\n",
    "        self.by = np.zeros((output_size,1))\n",
    "        self.bf = np.zeros((hidden_size,1))\n",
    "        self.bi = np.zeros((hidden_size,1))\n",
    "        self.bc = np.zeros((hidden_size,1))\n",
    "        self.bo = np.zeros((hidden_size,1))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = []\n",
    "        # Initialize arrays h and c\n",
    "        self.hs = []\n",
    "        self.cs = []\n",
    "        # Initialize for gates\n",
    "        self.fs = []\n",
    "        self.iss = []\n",
    "        self.os = []\n",
    "        # css is the array for the gate c tilde\n",
    "        self.css = []\n",
    "\n",
    "        h = np.zeros((self.Wf.shape[0], 1))\n",
    "        self.hs.append(h)\n",
    "        c = np.zeros((self.Wf.shape[0], 1))\n",
    "        self.cs.append(c)\n",
    "\n",
    "        # Now compute\n",
    "        for t, x in enumerate(inputs):\n",
    "            x = np.array(x)\n",
    "            x = x.reshape(-1,1)\n",
    "            self.inputs.append(x)\n",
    "            f = sigmoid(self.Uf @ self.hs[t] + self.Wf @ x + self.bf)\n",
    "            c_tilde = np.tanh(self.Uc @ self.hs[t] + self.Wc @ x + self.bc)\n",
    "            i = sigmoid(self.Ui @ self.hs[t] + self.Wi @ x + self.bi)\n",
    "            o = sigmoid(self.Uo @ self.hs[t] + self.Wo @ x + self.bo)\n",
    "            # Compute the memory cell\n",
    "            c = hadamard(c_tilde, i) + hadamard(self.cs[t], f)\n",
    "            # Compute the hidden state\n",
    "            h = hadamard(o, np.tanh(c))\n",
    "            # Append values for memory cell and hidden state\n",
    "            self.hs.append(h)\n",
    "            self.cs.append(c)\n",
    "            # Append values for gates\n",
    "            self.css.append(c_tilde)\n",
    "            self.fs.append(f)\n",
    "            self.os.append(o)\n",
    "            self.iss.append(i)\n",
    "        \n",
    "        n = len(inputs)\n",
    "        y_pred = self.Wy @ self.hs[n] + self.by\n",
    "        self.outputs = softmax(y_pred)\n",
    "    \n",
    "    def backward(self, dy, learning_rate = 0.03):\n",
    "        n = len(self.inputs)\n",
    "        dby = dy\n",
    "        dWy = dy @ self.hs[n].T\n",
    "        # That is dL/dh[n]\n",
    "        dh = self.Wy.T @ dy\n",
    "\n",
    "        dWf = np.zeros_like(self.Wf)\n",
    "        dWi = np.zeros_like(self.Wi)\n",
    "        dWo = np.zeros_like(self.Wo)\n",
    "        dWc = np.zeros_like(self.Wc)\n",
    "\n",
    "        dUf = np.zeros_like(self.Uf)\n",
    "        dUi = np.zeros_like(self.Ui)\n",
    "        dUo = np.zeros_like(self.Uo)\n",
    "        dUc = np.zeros_like(self.Uc)\n",
    "\n",
    "        dbf = np.zeros_like(self.bf)\n",
    "        dbi = np.zeros_like(self.bi)\n",
    "        dbo = np.zeros_like(self.bo)\n",
    "        dbc = np.zeros_like(self.bc)\n",
    "\n",
    "        for t in reversed(range(n-1)):\n",
    "            tmpf = self.fs[t+1] * (1 - self.fs[t+1])\n",
    "            tmpi = self.iss[t+1] * (1 - self.iss[t+1])\n",
    "            tmpo = self.os[t+1] * (1 - self.os[t+1])\n",
    "\n",
    "            tmpc = 1 - np.tanh(self.cs[t])**2\n",
    "            tmpc_tilde = 1 - np.tanh(self.css[t+1])**2\n",
    "\n",
    "            dhtdct = self.os[t+1] @ tmpc.T\n",
    "            dhtdot = np.tanh(self.cs[t+1])\n",
    "\n",
    "            dotdht_1 = self.Uo @ tmpo\n",
    "            dotdUo = tmpo @ self.hs[t].T\n",
    "            dotdWo = tmpo @ self.inputs[t+1].T\n",
    "            dotdbo = np.sum(tmpo, axis=1, keepdims=True)\n",
    "\n",
    "            dctdct_1 = self.fs[t+1]\n",
    "            dctdft = self.cs[t]\n",
    "            dctdit = self.css[t+1]\n",
    "            dctdct_tilde = self.iss[t+1]\n",
    "\n",
    "            dftdUf = tmpf @ self.hs[t].T\n",
    "            dftdht_1 = self.Uf @ tmpf\n",
    "            dftdWf = tmpf @ self.inputs[t+1].T\n",
    "            dftdbf = np.sum(tmpf, axis = 1, keepdims=True)\n",
    "\n",
    "            ditdUi = tmpi @ self.hs[t].T\n",
    "            ditdht_1 = self.Ui @ tmpi\n",
    "            ditdWi = tmpi @ self.inputs[t+1].T\n",
    "            ditdbi = np.sum(tmpi, axis = 1, keepdims=True)\n",
    "\n",
    "            dc_tildedUc = tmpc_tilde @ self.hs[t].T\n",
    "            dc_tildedht_1 = self.Uc @ tmpc_tilde\n",
    "            dc_tildedWc = tmpc_tilde @ self.inputs[t+1].T\n",
    "            dc_tildedbc = np.sum(tmpc_tilde, axis = 1, keepdims = True)\n",
    "            \n",
    "\n",
    "            dhtdht_1 = dhtdct @ dctdct_tilde @ dc_tildedht_1.T + dhtdct @ dctdft @ dftdht_1.T + dhtdct @ dctdit @ ditdht_1.T + dhtdot @ dotdht_1.T\n",
    "            # temp is dL/dht\n",
    "            temp = dhtdht_1 @ dh\n",
    "\n",
    "            dUo += temp @ (dhtdot.T @ dotdUo)\n",
    "            dWo += temp @ (dhtdot.T @ dotdWo)\n",
    "            dbo += temp @ (dhtdot.T @ dotdbo)\n",
    "\n",
    "            dUf += temp @ (dhtdct @ dctdft).T @ dftdUf\n",
    "            dWf += temp @ (dhtdct @ dctdft).T @ dftdWf\n",
    "            dbf += temp @ (dhtdct @ dctdft).T @ dftdbf\n",
    "\n",
    "            dUi += temp @ (dhtdct @ dctdit).T @ ditdUi\n",
    "            dWi += temp @ (dhtdct @ dctdit).T @ ditdWi\n",
    "            dbi += temp @ (dhtdct @ dctdit).T @ ditdbi\n",
    "\n",
    "            dUc += temp @ (dhtdct @ dctdct_tilde).T @ dc_tildedUc\n",
    "            dWc += temp @ (dhtdct @ dctdct_tilde).T @ dc_tildedWc\n",
    "            dbc += temp @ (dhtdct @ dctdct_tilde).T @ dc_tildedbc\n",
    "\n",
    "            # Update dh and clip its values\n",
    "            dh = temp\n",
    "            for d in [dh]:\n",
    "                np.clip(d, 1e-15, 1 - 1e-15, out = d)\n",
    "        \n",
    "\n",
    "        # Clip to prevent gradient vanishing/exploding\n",
    "        for d in [dUo, dWo, dbo, dUf, dWf, dbf, dUi, dWi, dbi, dUc, dWc, dbc, dby, dWy]:\n",
    "            np.clip(d, -1, 1, out = d)\n",
    "\n",
    "        # Update parameters\n",
    "        self.Uf -= learning_rate * dUf\n",
    "        self.Wf -= learning_rate * dWf\n",
    "        self.bf -= learning_rate * dbf\n",
    "\n",
    "        self.Wi -= learning_rate * dWi\n",
    "        self.Ui -= learning_rate * dUi\n",
    "        self.bi -= learning_rate * dbi\n",
    "\n",
    "        self.Wo -= learning_rate * dWo\n",
    "        self.Uo -= learning_rate * dUo\n",
    "        self.bo -= learning_rate * dbo\n",
    "\n",
    "        self.Wc -= learning_rate * dWc\n",
    "        self.Uc -= learning_rate * dUc\n",
    "        self.bc -= learning_rate * dbc\n",
    "\n",
    "        self.by -= learning_rate * dby\n",
    "        self.Wy -= learning_rate * dWy\n",
    "\n",
    "    def process(self, data, learning_rate = 0.03):\n",
    "        accuracy = 0\n",
    "        for i, x in enumerate(data):\n",
    "            inputs = x\n",
    "            self.forward(inputs)\n",
    "            probs = self.outputs\n",
    "            # Accuracy\n",
    "            accuracy += (np.argmax(probs) == y_true[i])\n",
    "            \n",
    "            dy = probs\n",
    "            dy[y_true[i]] -= 1\n",
    "            # Backward\n",
    "            self.backward(dy, learning_rate)\n",
    "        \n",
    "        self.accuracy = float(accuracy/len(data))\n",
    "        \n",
    "    def fit(self, data, max_iter = 201, learning_rate = 0.03):\n",
    "        for i in range(max_iter):\n",
    "            self.process(data, learning_rate)\n",
    "            if(i % 20 == 0):\n",
    "                print(f\"Step: {i}\")\n",
    "                print(f\"accuracy for training data: {self.accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n",
      "accuracy for training data: 0.15263157894736842\n",
      "Step: 20\n",
      "accuracy for training data: 0.17894736842105263\n",
      "Step: 40\n",
      "accuracy for training data: 0.21578947368421053\n",
      "Step: 60\n",
      "accuracy for training data: 0.20526315789473684\n",
      "Step: 80\n",
      "accuracy for training data: 0.18421052631578946\n",
      "Step: 100\n",
      "accuracy for training data: 0.22631578947368422\n",
      "Step: 120\n",
      "accuracy for training data: 0.28421052631578947\n",
      "Step: 140\n",
      "accuracy for training data: 0.2736842105263158\n",
      "Step: 160\n",
      "accuracy for training data: 0.3263157894736842\n",
      "Step: 180\n",
      "accuracy for training data: 0.3473684210526316\n",
      "Step: 200\n",
      "accuracy for training data: 0.3736842105263158\n"
     ]
    }
   ],
   "source": [
    "lstm = LSTM(vocab_size, vocab_size)\n",
    "lstm.fit(X, learning_rate=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
